---
layout: post
title: 深度学习实践经验总结
tags: deep_learning
toc: true
math: true
date: 2021-08-24 21:00 +0800
---


## 一、train accuracy不高的技巧

### 1. 选择合适的loss function：

不同的loss function训练出来的模型精度可能会有很大差别，比如Square Error可能训练模型的精度很低，但Cross Entropy训练出来的模型的精度较高。

这是因为对于某个问题，不同loss计算出来的total loss分布会不一样，Square Error计算出来的total loss高低落差没有很大，梯度下降很难找到最佳解，因此训练精度很难提高。但Cross Entropy得到的loss function的高低落差就很大，方便梯度下降找到最佳解。
    
### 2. mini-batch

调整batch size。

### 3. New activation function

调整激活函数，这是因为比如sigmod激活函数会把每次的输出限定在0~1之间，这样求梯度反向传导的时候，在网络前面的参数的改变会很小，导致网络后面的参数都达到了收敛，但前面的参数是随机的，这样的网络是不work的，这样也是梯度消失的问题。

如果把激活函数改为relu激活函数，小于0的值不传播，大于0的值按原本值传播，也就一定程度上减少了梯度消失的问题。

### 4. Adaptive learning rate

缘由如果学习率太大，一直在震荡，一直收敛不了，如果学习率太大，训练的很慢，要走很久。

这里以AdaGrad为例来讲解，逐渐衰减的learning rate。
original: w <- w-n*(L在w上的梯度)
Adagrad:  w <- w-nw(L在w上的梯度)
nw是个变量，是n/（若干次epoch之后的梯度的总和）
这里要注意两点：
(1 训练越久，分母越大，学习率就越小；
(2 梯度总和越小，学习率越大；
第二点解释为，梯度越小相当于平坡，则可以认为训练的步伐可以大些，对应就是学习率可以较大；

其他的自适应学习率调整的方法有：
RMSprop, Adadelta, "No more pesky learning rates", AdaSecant, Adam, Nadam

### 5. Momentum

为了解决不卡在local minima的问题，引入momentum的概念，可以理解为给梯度下降一个惯性，让原本的结果往前多走一些，也是给于找到global minima一些希望。


## 二、test accuracy不高的技巧(overfitting)

万能方法：有更多的training data，可用数据增强方法创建数据集，比如简单的旋转角度，翻转等空间图像处理的技巧。

### 1. Early Stopping

核心意思就是随着训练epoch数越多，会出现训练数据集的误差越小，但是验证数据集的误差会增大，此时应该提前结束训练，否则就会出现overfitting的现象。


### 2. Weight Decay

核心思想就是有些用不到节点，将该节点萎缩（使得w很小）

original:     w <- w-n*(L在w上的梯度)
wight decay:  w <- (0.99)w-n*(L在w上的梯度)

这里w会乘很多的0.99，如果是没用到的节点，则会越来越小、接近于零，用到的节点则不会。

### 3. Dropout

每一层随机有P%的节点dropout(丢掉)。--> 实际上是改变了网络的结构

注意在测试的时候，不要去dropout,对于所有的weights times要乘以(1-p%)。

实验效果：training accuracy会降低，testing accuracy会提升。

###（Regularization）


### 4. Network Structure

## 三、深度学习神经网络的整个流程

step1: 设计好网络架构

step2: 定义评估方法

step3: 找到最佳的解

然后就有 Neural Network

训练之后在Training Data看下是否好的结果，否则训练精度不足到step3

之后在Testing Data看是否有好的结果，否则训练过拟合到step1

都没问题则训练成功。


参考链接：https://www.bilibili.com/video/BV1Ex411475k?from=search&seid=300313369636269645&spm_id_from=333.337.0.0

