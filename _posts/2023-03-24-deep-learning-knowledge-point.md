---
layout: post
title: 深度学习知识点
tags: deep learning
toc: true
date: 2023-03-24 21:00 +0800
---


参考：https://leetcode.cn/leetbook/read/da-han-suan-fa-gang-ti-mu-he-ji-shang/nlhvl1/


## 1. BN 过程，为什么测试和训练不一样？（字节跳动）

对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。
对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

## 2. 简单介绍 gbdt 算法的原理（美团）

GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值作为残差的近似值，从而拟合一个回归树。**对于分类问题：常使用指数损失函数；对于回归问题：常使用平方误差损失函数（此时，其负梯度就是通常意义的残差），**对于一般损失函数来说就是残差的近似。 无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损失函数是均方损失时，负梯度刚好是残差，残差只是特例。

## 3. pca 属于有监督还是无监督？（美团）

PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。

主成分分析 (PCA, principal component analysis)是一种数学降维方法, 利用正交变换 (orthogonal transformation)把一系列可能线性相关的变量转换为一组线性不相关的新变量，也称为主成分，从而利用新变量在更小的维度下展示数据的特征。

实现过程：
一种是基于特征值分解协方差矩阵实现PCA算法，一种是基于SVD分解协方差矩阵实现PCA算法。
意义：

使得数据集更易使用；降低算法的计算开销；去除噪声；使得结果容易理解。

## 4. 防止过拟合的方法？（阿里）

- 降低模型复杂度
- 增加更多的训练数据：使用更大的数据集训练模型
- 数据增强
- 正则化：L1、L2、添加BN层
- 添加Dropout策略
- Early Stopping
- 创建一个验证集是最基本的防止过拟合的方法。我们最终训练得到的模型目标是要在验证集上面有好的表现，而不训练集
- 交叉验证
- 特征选择/特征降维

## 5. Pytorch 和 Tensorflow 的区别？（科大讯飞）

- 图创建

创建和运行计算图可能是两个框架最不同的地方。

pyTorch：图结构是动态的，这意味着图在运行时构建TensorFlow：图结构是静态的，这意味着图先被“编译”然后再运行。

pyTorch中简单的图结构更容易理解，更重要的是，还更容易调试。调试pyTorch代码就像调试Python代码一样。你可以使用pdb并在任何地方设置断点。

调试tensorFlow代码可不容易。要么得从会话请求要检查的变量，要么学会使用tensorFlow的调试器。

- 灵活性

pytorch：动态计算图，数据参数在CPU与GPU之间迁移十分灵活，调试简便；
tensorflow：静态计算图，数据参数在CPU与GPU之间迁移麻烦，调试麻烦。

- 设备管理

pytorch：需要明确启用的设备
tensorflow：不需要手动调整，简单

## 6. torch.eval() 的作用？（科大讯飞）

实际为 model.eval(),直观的理解为model的推理模式；

- 对BN的影响：

对于BN，训练时通常采用mini-batch，所以每一批中的mean和std大致是相同的；而测试阶段往往是单个图像的输入，不存在mini-batch的概念。所以将model改为eval模式后，BN的参数固定，并采用之前训练好的全局的mean和std；总结就是使用全局固定的BN。

- 对dropout的影响：

训练阶段，隐含层神经元先乘概率P，再进行激活；而测试阶段，神经元先激活，每个隐含层神经元的输出再乘概率P，总结来说就是顺序不同！

## 7. 工业界中遇到上亿的图像检索任务，如何提高图像对比效率?（其他）

假设原图像输出的特征维度为2048维，通过哈希的索引技术，将原图的2048维度映射到128维度的0/1值中，再进⾏特征维度对⽐。

## 8. KNN 中的 K 如何选取的？（BAT）
- 如果选择较小的K值，容易发生过拟合；
- 如果选择较大的K值，预测易发生错误，且K值的增大就意味着整体的模型变得简单；
- 在实际应用中，K值一般取一个比较小的数值，例如采用**交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）**来选择最优的K值。

## 9. 请问（决策树、Random Forest、Booting、Adaboot）GBDT 和 XGBoost 的区别是什么？（BAT）
xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：
1. 损失函数是用泰勒展式**二项逼近**，而不是像gbdt里的就是一阶导数
2. 对树的结构进行了**正则化约束**，防止模型过度复杂，降低了过拟合的可能性
3. 节点分裂的方式不同，gbdt是用的gini系数，xgboost是**经过优化推导后的**

## 10. 为什么 XGBoost 要用泰勒展开，优势在哪里？（BAT）
XGBoost使用了一阶和二阶偏导，**二阶导数有利于梯度下降的更快更准**。使用泰勒展开取得二阶倒数形式，可以在不选定损失函数具体形式的情况下用于算法优化分析。本质上也就把损失函数的选取和模型算法优化/参数选择分开了。这种去耦合增加了XGBoost的适用性。

## 11. XGBoost 如何寻找最优特征？是又放回还是无放回的呢？（BAT）
XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性。XGBoost利用梯度优化模型算法，样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴)。但XGBoost支持子采样，也就是每轮计算可以不使用全部样本。

## 12. L1 和 L2 的区别。（BAT）

- L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。
比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|
- 简单总结一下就是：
   - L1范数: 为x向量各个元素绝对值之和，L1范数可以使权值稀疏，方便特征提取，L1是拉普拉斯分布。
   - L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数，L2范数可以防止过拟合，提升模型的泛化能力，L2是高斯分布。
   - Lp范数: 为x向量各个元素绝对值p次方和的1/p次方
在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。

## 13. LSTM 结构推导，为什么比 RNN 好？（BAT）
因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。

## 14. 如何解决梯度消失和梯度膨胀？（BAT）

（1）梯度消失：
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。
可以采用ReLU激活函数有效的解决梯度消失的情况，因为ReLu激活函数的导数: ReLU(x)′={0,ifx<0; 1,ifx>0}。
参考：https://blog.csdn.net/akadiao/article/details/78247449
https://zhuanlan.zhihu.com/p/180568816
（2）梯度膨胀：
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。
可以通过激活函数来解决。

## 15. 了解正则化吗？（BAT）

正则化为了解决过拟合的问题。
////

## 16. BN和LN的区别

BN--BatchNormalization：对这个batch的样本同一纬度的特征算均值和方差，归一化处理，一般用于固定shape场景；

LN--LayerNormalization：对这单个样本的所有特征做归一化，一般用于动态shape场景，如NLP任务；

## 17. 比较 Boosting 和 Bagging 的异同（其他）

bagging：有放回样本地抽取数据集，训练k个弱学习器，将这k个弱学习器以投票的方式得到最终的分类结果。

boosting：每一轮根据上一轮的分类结果动态调整每个样本在分类器中的权重，简单样本权重减弱，难例样本权重增强，让分类器关注难例样本，训练k个弱分类器，分类器也有权重，精度高的权重高，通过加权组合的方式得到最终的分类结果。

## 18. 什么是RNN？（其他）

RNN的目的是是用来处理序列数据的。传统的神经网络，隐藏层之间的节点是没有连接的，这就导致很多问题解决不了，比如NLP问题，预测的内容和上下文有关。RNN中隐藏层之间的节点就会有连接，引入上下文的信息，在处理NLP任务时，使得任务更加符合智能的逻辑，对历史的文字有“记忆”。

## 19. 卷积神经网络 CNN 中池化层有什么作用？（其他）

减小图像尺寸即数据降维，缓解过拟合，保持一定程度的旋转和平移不变性。

## 20. 神经网络中 Dropout 的作用？具体是怎么实现的？（其他）

防止过拟合。每次训练，都对每个神经网络单元，按一定概率临时丢弃。

## 21.利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？（其他）

(1) 学习率太小，排查学习率的问题；
(2) 梯度消失，改变激活函数或改变权值的初始化；

## 22.残差网络为什么能做到很深层？（其他）

NN在反向传播时，由于链式求导法则，层数越深，梯度在逐层传播过程中会逐渐衰减，导致无法对前面网络层的权重进行有效的调整。残差网络会添加short connections为梯度带来直接向前面层的传播通道，缓解了梯度的减小问题。

## 23.卷积神经网络中空洞卷积的作用是什么？（其他）

空洞卷积也叫扩张卷积，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为 2 的 3×3 卷积核，感受野与 5×5 的卷积核相同，但参数数量仅为 9 个。



